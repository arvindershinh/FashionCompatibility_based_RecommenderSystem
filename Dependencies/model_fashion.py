# -*- coding: utf-8 -*-
"""Model_Fashion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A5jSoNN65PXy2aRwH1pO4la4bgkNrw-5
"""

# !pip install git+https://github.com/deepmind/dm-haiku

# import outfitprocessing as data
import bi_lstm_fashion as bi_lstm
import visualsemantic_fashion as VisualSemantic

from typing import Iterator, Mapping

import numpy as np
import haiku as hk
import jax
import jax.numpy as jnp

# OutfitBatchData = data.sequence_Tensor
# OutfitBatchData['outfitSequencesImage'].shape, OutfitBatchData['outfitSequencesCaption'].shape

def total_loss(batch):
  image_Tensor, caption_Tensor = batch['outfitSequencesImage'], batch['outfitSequencesCaption']

  B, S, E = image_Tensor.shape
  image_Tensor_biLSTM = jnp.reshape(image_Tensor, (S, B, E))

  def visualSemanticInput(x):
    img_Tensor, captn_Tensor = x
    images   = jnp.reshape(img_Tensor, (-1, img_Tensor.shape[-1]))
    captions = jnp.reshape(captn_Tensor, (-1, captn_Tensor.shape[-1]))
    captions_Len = jnp.reshape(jnp.sum(captions, axis=1), (-1,1))
    return {'img_batch' : images, 'caption_batch' : captions/captions_Len}

  VisualSemanticData = visualSemanticInput((image_Tensor, caption_Tensor))

  objF_objB_objVS = bi_lstm.sequence_loss(image_Tensor_biLSTM)+VisualSemantic.visual_semantic_fn(VisualSemanticData)

  return objF_objB_objVS

# batch_lstm_init = next(data_py.train_data)
# batch_lstm = next(data_py.train_data)

# batch_vs_init = {'images' : v1, 'captions' : s1}
# batch_vs = {'images' : v1, 'captions' : s1}

# params_init, loss_fn  = hk.without_apply_rng(hk.transform(total_loss))
# loss_fn = jax.jit(loss_fn)

# rng = hk.PRNGSequence(42)
# initial_params = params_init(next(rng), OutfitBatchData)

# gradients = jax.grad(loss_fn)(initial_params, OutfitBatchData)
# gradients.keys()

# gradients['visual_semantic'].keys()

# gradients['lstm/linear'].keys()

"""CACHE"""

# a,b = total_loss(OutfitBatchData)

# a.shape, b['img_batch'].shape, b['caption_batch'].shape

# batch_lstm = next(data_py.train_data)

# batch_lstm.keys(), type(batch_lstm['input']), type(batch_lstm['target'])

# batch_vs = {'images' : v1, 'captions' : s1}

# batch_vs.keys(), type(batch_vs['images']), type(batch_vs['captions'])